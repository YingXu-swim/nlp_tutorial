{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "29a2e30b7fc2aef9061214cc512c228f",
     "grade": false,
     "grade_id": "cell-e823893319cc2093",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "ELEC-E5550 - Statistical Natural Language Processing\n",
    "# SET 5: Machine Translation Evaluation\n",
    "\n",
    "# Released: 07.03.2023\n",
    "# Deadline: 20.03.2023 at midnight"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "871c0bff5de00c3a7fb59ac04c9059ab",
     "grade": false,
     "grade_id": "cell-2b4616209ec110f7",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "In this assignment, you will learn how machine translation systems can be automatically evaluated, using the BLEU score.\n",
    "\n",
    "KEYWORDS:\n",
    "* BLEU\n",
    "\n",
    "Do not use external libraries in this assignment. Python standard library is allowed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "8d058bcc1b883e9aee84f50f123571ab",
     "grade": false,
     "grade_id": "cell-e1d385c1c09cab25",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "# Table of contents\n",
    "\n",
    "* [TASK 1: BLEU score](#task1)\n",
    "    * [Step 1.1: Match N-grams](#subtask1_1)\n",
    "    * [Step 1.2: Match lengths](#subtask1_2)\n",
    "    * [Step 1.3: Put it together](#subtask1_3)\n",
    "    * [Step 1.4: Reflect on BLEU](#subtask1_4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "604663eb04e8bfb8cbe844d42fde780b",
     "grade": false,
     "grade_id": "cell-4f67e9fc3d6c389a",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Evaluation is a particularly difficult topic in machine translation. Human evaluations are preferred, but expensive. There are multiple automatic evaluation metrics in use. The most common one is BLEU. In this assignment you will implement BLEU calculation - it is computationally very simple.\n",
    "\n",
    "## TASK 1 <a class=\"anchor\" id=\"task1\"></a>\n",
    "## BLEU score\n",
    "\n",
    "**BLEU**, or bilingual evaluation understudy, was proposed in 2002, here: https://www.aclweb.org/anthology/P02-1040.pdf. Take read through the paper, it is clearly written and provides not just the algorithm, but also context and reasons for the design choices. We will implement the algorithm as described by the paper, so you can refer to it.\n",
    "\n",
    "### High-level overview\n",
    "\n",
    "The essential requirement for BLEU is an annotated test corpus. The test corpus consists of source language segments and multiple (or at least one) reference translations for each segment. The reference translations are made by human translators. Multiple reference translations are used to account for the fact that there is no single correct translation.\n",
    "\n",
    "A machine translation system produces a hypothesis translation for each source segment. The hypothesis is then compared to all the references. BLEU looks for matching N-grams of different lengths N. The more matches, the better. Additionally, BLEU compares the length of the hypothesis with the lengths of the translations - brevity is penalized. These two components are combined into a score between 0 and 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "cd14ed4a52b7d407d59494a4fc0d0002",
     "grade": false,
     "grade_id": "cell-5a5cd9018823435c",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## 1.1 Match N-grams (3 Points) <a class=\"anchor\" id=\"subtask1_1\"></a>\n",
    "Specifically, BLEU uses a concept called modified precision. The hypothesis n-grams can find a match in any reference - but if the same n-gram occurs multiple times in the hypothesis, the number of matches is clipped to the maximum number of times the n-gram occurs in any single reference. The intuition is simple - the references are likely to have many words and phrases in common. Without the clipping, repeating the same likely words in the hypothesis would yield an inflated BLEU score. The hypothesis \"the the the the the the the\" would find many matches from almost any set of references."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "9107096988cfded90303e65a8eecf6d4",
     "grade": false,
     "grade_id": "cell-7830e8ece39c4315",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('furiously', 'sleep', 'ideas'), ('sleep', 'ideas', 'green'), ('ideas', 'green', 'colorless')]\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "You may use this ngram forming implementation in your solution.\n",
    "This returns a generator. If you absolutely need a list, you can\n",
    "simply call\n",
    ">>> list(form_ngrams(...))\n",
    "\"\"\"\n",
    "from collections import deque\n",
    "\n",
    "def form_ngrams(tokens, n):\n",
    "    \"\"\"Forms all ngrams from tokens\n",
    "    \n",
    "    Arguments\n",
    "    ---------\n",
    "    tokens : iterable\n",
    "        Tokens to form n-grams from\n",
    "    n : int\n",
    "        The length of ngrams to form\n",
    "    \n",
    "    Yields\n",
    "    ------\n",
    "    tuple\n",
    "        Yields each ngram as a tuple of tokens.   \n",
    "    \"\"\"\n",
    "    window = deque(maxlen=n)\n",
    "    for i, token in enumerate(tokens, start=1):\n",
    "        window.append(token)\n",
    "        if i >= n:\n",
    "            yield tuple(window)\n",
    "            \n",
    "print(list(form_ngrams([\"furiously\", \"sleep\", \"ideas\", \"green\", \"colorless\"], 3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "c437283958ad8f06d1bee2e5674be686",
     "grade": false,
     "grade_id": "cell-c17a696f8678fb21",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "def ngram_count_times(word, sentence):\n",
    "    \"\"\"return how many times the n-gram appears in this sentence\"\"\"\n",
    "    n = len(word)\n",
    "    length = len(sentence)\n",
    "    \n",
    "    cnt = 0\n",
    "    \n",
    "    s_idx = 0\n",
    "    while 1:\n",
    "        e_idx = s_idx + n\n",
    "        if e_idx > length: break\n",
    "        if sentence[s_idx:e_idx] == list(word):\n",
    "            cnt += 1\n",
    "        s_idx += 1\n",
    "    return cnt\n",
    "\n",
    "def modified_precision(hypothesis, references, n):\n",
    "    ########################################\n",
    "    # need to read paper and implement according to the formule in the paper\n",
    "    \"\"\"Matches hypothesis N-grams to references at given length N\n",
    "    \n",
    "    Hypothesis and references are given as plain token sequences. This function\n",
    "    forms N-grams of appropriate lengths.\n",
    "    \n",
    "    Count the number of times each N-gram occurs in the hypothesis.\n",
    "    Then take the union of N-gram counts in all the references:\n",
    "    for each N-gram, keep track of the maximum number of times it occurs in a single\n",
    "    reference. This forms the reference counts.\n",
    "    Finally, count the clipped hits - sum each N-gram count in the hypothesis,\n",
    "    but clip those hypothesis counts to the reference counts.\n",
    "    \n",
    "    This returns the hits and number of candidates separately, since those values\n",
    "    are aggregated over the full corpus in BLEU.\n",
    "    The modified precision for this single segment would be clipped_hits / num_candidates\n",
    "    \n",
    "    Arguments\n",
    "    ---------\n",
    "    hypothesis : list of str\n",
    "        List of tokens - the hypothesis translation.\n",
    "        E.G. [\"alo\", \"mundo\"]\n",
    "    references : list of lists of str\n",
    "        List of references - and each reference is a list of tokens.\n",
    "        E.G. [[\"hola\", \"mundo\"], [\"alo\", \"tierra\"]]\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    int\n",
    "        The number of candidate N-grams of length N in the hypothesis that found\n",
    "        a match.\n",
    "    int\n",
    "        The total number of candidate N-grams of length N in the hypothesis.\n",
    "    \"\"\"\n",
    "    \n",
    "    hyp_counts = Counter(form_ngrams(hypothesis,n))\n",
    "    num_candidates = sum(hyp_counts.values()) # how many keys this counter has\n",
    "    if num_candidates == 0:\n",
    "        # If hypothesis is shorter than N, we get here.\n",
    "        # We return 0, 1 instead of 0, 0, so that we don't\n",
    "        # run into divide-by-zero errors.\n",
    "        return 0, 1\n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "    # raise NotImplementedError()\n",
    "    clipped_hits = 0\n",
    "    \n",
    "    max_count_each_ref_ls = []\n",
    "    for ngram in hyp_counts:\n",
    "        count_each_ref_ls = []\n",
    "        for ref in references: # for each reference\n",
    "            count_each_ref = ngram_count_times(ngram, ref)\n",
    "            count_each_ref_ls.append(count_each_ref)\n",
    "            count_hyp = ngram_count_times(ngram, hypothesis)\n",
    "        clipped_hits += min(count_hyp, max(count_each_ref_ls))\n",
    "            \n",
    "    # Return the numerator and denominator separately, since\n",
    "    # they get aggregated over the whole corpus.\n",
    "    return clipped_hits, num_candidates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "d80b30f9e5f609aba6edd595b534b098",
     "grade": true,
     "grade_id": "cell-abdd55f7362d275f",
     "locked": true,
     "points": 3,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "from nose.tools import assert_equal\n",
    "\n",
    "# Basic sanity check:\n",
    "assert_equal(modified_precision([\"a\",\"b\",\"c\",\"d\"], [[\"a\",\"b\",\"c\",\"d\"]], 1), (4, 4))\n",
    "assert_equal(modified_precision([\"a\",\"b\",\"c\",\"d\"], [[\"a\",\"b\",\"c\",\"d\"]], 2), (3, 3))\n",
    "assert_equal(modified_precision([\"a\",\"b\",\"c\",\"d\"], [[\"a\",\"b\",\"c\",\"d\"]], 3), (2, 2))\n",
    "assert_equal(modified_precision([\"a\",\"b\",\"c\",\"d\"], [[\"a\",\"b\",\"c\",\"d\"]], 4), (1, 1))\n",
    "\n",
    "# Small difference:\n",
    "assert_equal(modified_precision([\"a\",\"b\",\"c\",\"d\"], [[\"a\",\"b\",\"c\",\"e\"]], 1), (3, 4))\n",
    "assert_equal(modified_precision([\"a\",\"b\",\"c\",\"d\"], [[\"a\",\"b\",\"c\",\"e\"]], 2), (2, 3))\n",
    "assert_equal(modified_precision([\"a\",\"b\",\"c\",\"d\"], [[\"a\",\"b\",\"c\",\"e\"]], 3), (1, 2))\n",
    "assert_equal(modified_precision([\"a\",\"b\",\"c\",\"d\"], [[\"a\",\"b\",\"c\",\"e\"]], 4), (0, 1))\n",
    "\n",
    "# More references:\n",
    "assert_equal(modified_precision([\"a\",\"b\",\"c\",\"d\"], [[\"a\",\"b\",\"c\",\"e\"],[\"b\",\"c\",\"c\",\"d\"]], 1), (4, 4))\n",
    "assert_equal(modified_precision([\"a\",\"b\",\"c\",\"d\"], [[\"a\",\"b\",\"c\",\"e\"],[\"b\",\"c\",\"c\",\"d\"]], 2), (3, 3))\n",
    "assert_equal(modified_precision([\"a\",\"b\",\"c\",\"d\"], [[\"a\",\"b\",\"c\",\"e\"],[\"b\",\"c\",\"c\",\"d\"]], 3), (1, 2))\n",
    "assert_equal(modified_precision([\"a\",\"b\",\"c\",\"d\"], [[\"a\",\"b\",\"c\",\"e\"],[\"b\",\"c\",\"c\",\"d\"]], 4), (0, 1))\n",
    "\n",
    "# Clipping:\n",
    "assert_equal(modified_precision([\"c\",\"c\",\"c\",\"c\"], [[\"a\",\"b\",\"c\",\"e\"],[\"b\",\"c\",\"c\",\"d\"]], 1), (2, 4))\n",
    "assert_equal(modified_precision([\"c\",\"c\",\"c\",\"c\"], [[\"a\",\"b\",\"c\",\"e\"],[\"b\",\"c\",\"c\",\"d\"]], 2), (1, 3))\n",
    "assert_equal(modified_precision([\"c\",\"c\",\"c\",\"c\"], [[\"a\",\"b\",\"c\",\"e\"],[\"b\",\"c\",\"c\",\"d\"]], 3), (0, 2))\n",
    "assert_equal(modified_precision([\"c\",\"c\",\"c\",\"c\"], [[\"a\",\"b\",\"c\",\"e\"],[\"b\",\"c\",\"c\",\"d\"]], 4), (0, 1))\n",
    "\n",
    "# With real text:\n",
    "assert_equal(modified_precision([\"waters\", \"destroyed\", \"perfume\"], \n",
    "                                [[\"the\", \"city\", \"of\", \"cologne\", \"was\", \"destroyed\", \"by\", \"flood\", \"waters\"]], \n",
    "                                1), (2, 3))\n",
    "assert_equal(modified_precision([\"waters\", \"destroyed\", \"perfume\"], \n",
    "                                [[\"the\", \"city\", \"of\", \"cologne\", \"was\", \"destroyed\", \"by\", \"flood\", \"waters\"]], \n",
    "                                2), (0, 2))\n",
    "\n",
    "austen_hypo = [\"it\", \"is\", \"a\", \"widely\", \"accepted\", \"truth\", \n",
    "              \"that\", \"an\", \"unmarried\", \"wealthy\", \"man\", \n",
    "              \"necessarily\", \"needs\", \"a\", \"wife\", \"alongside\", \"him\"]\n",
    "austen_ref = [[\"it\", \"is\", \"a\", \"truth\", \"universally\", \"acknowledged\",\n",
    "               \"that\", \"a\", \"single\", \"man\", \"in\", \"posession\", \"of\",\"a\"\"good\",\"fortune\",\n",
    "               \"must\", \"be\", \"in\", \"want\", \"of\", \"a\", \"wife\"],\n",
    "              [\"everybody\", \"knows\", \"the\",\"rich\",\"man\",\"necessarily\", \"needs\", \"a\", \"wife\"]]\n",
    "assert_equal(modified_precision(austen_hypo, austen_ref, 3), (4,15))\n",
    "assert_equal(modified_precision(austen_hypo, austen_ref, 4), (2,14))\n",
    "assert_equal(modified_precision(austen_hypo, austen_ref, 5), (1,13))\n",
    "assert_equal(modified_precision(austen_hypo, austen_ref, 6), (0,12))\n",
    "\n",
    "assert_equal(modified_precision([\"waters\", \"destroyed\", \"perfume\"],\n",
    "                                [[\"the\", \"city\", \"of\", \"cologne\", \"was\", \"destroyed\", \"by\", \"flood\", \"waters\"]], \n",
    "                                1), (2, 3))\n",
    "assert_equal(modified_precision([\"waters\", \"destroyed\", \"perfume\"],\n",
    "                                [[\"the\", \"city\", \"of\", \"cologne\", \"was\", \"destroyed\", \"by\", \"flood\", \"waters\"]], \n",
    "                                2), (0, 2))\n",
    "assert_equal(modified_precision([\"waters\", \"destroyed\", \"perfume\"],\n",
    "                                [[\"the\", \"city\", \"of\", \"cologne\", \"was\", \"destroyed\", \"by\", \"flood\", \"waters\"]], \n",
    "                                3), (0, 1))\n",
    "assert_equal(modified_precision([\"waters\", \"destroyed\", \"perfume\"],\n",
    "                                [[\"the\", \"city\", \"of\", \"cologne\", \"was\", \"destroyed\", \"by\", \"flood\", \"waters\"]], \n",
    "                                4), (0, 1))\n",
    "\n",
    "assert_equal(modified_precision([\"the\", \"the\", \"the\", \"the\", \"the\", \"the\", \"the\"],\n",
    "                                [[\"the\", \"cat\", \"is\", \"on\", \"the\", \"mat\"], \n",
    "                                 [\"there\", \"is\", \"a\", \"cat\", \"on\", \"the\", \"mat\"]], 1), (2, 7))\n",
    "assert_equal(modified_precision([\"the\", \"the\", \"the\", \"the\", \"the\", \"the\", \"the\"],\n",
    "                                [[\"the\", \"cat\", \"is\", \"on\", \"the\", \"mat\"], \n",
    "                                 [\"there\", \"is\", \"a\", \"cat\", \"on\", \"the\", \"mat\"]], 2), (0, 6))\n",
    "assert_equal(modified_precision([\"the\", \"the\", \"the\", \"the\", \"the\", \"the\", \"the\"],\n",
    "                                [[\"the\", \"cat\", \"is\", \"on\", \"the\", \"mat\"], \n",
    "                                 [\"there\", \"is\", \"a\", \"cat\", \"on\", \"the\", \"mat\"]], 3), (0, 5))\n",
    "assert_equal(modified_precision([\"the\", \"the\", \"the\", \"the\", \"the\", \"the\", \"the\"],\n",
    "                                [[\"the\", \"cat\", \"is\", \"on\", \"the\", \"mat\"], \n",
    "                                 [\"there\", \"is\", \"a\", \"cat\", \"on\", \"the\", \"mat\"]], 4), (0, 4))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "9957de7709beea78c09779695d8c0000",
     "grade": false,
     "grade_id": "cell-68c5e02f9fade5ad",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## 1.2 Match lengths (1 Point) <a class=\"anchor\" id=\"subtask1_2\"></a>\n",
    "\n",
    "BLEU penalizes short hypotheses. This counter part is needed for precision, because precision by itself encourages adding only the most certain candidates. Typically recall is used as precision's pair, why not here? You can read the original paper to see. Instead of recall, BLEU uses a Brevity Penalty, which is computed over the whole test corpus. Here we just find the values which are aggregated - the hypothesis length and the closest reference length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "78ffbf4094d18eee16f0c831294fa900",
     "grade": false,
     "grade_id": "cell-137382ee46d4f47f",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def brevity_penalty_values(hypothesis, references):\n",
    "    \"\"\"Computes the length of the hypothesis and find the closest reference length.\n",
    "    \n",
    "    Pick the shortest reference length if there are many equally close\n",
    "    \n",
    "    Arguments\n",
    "    ---------\n",
    "    hypothesis : list of str\n",
    "        List of tokens - the hypothesis translation.\n",
    "        E.G. [\"alo\", \"mundo\"]\n",
    "    references : list of lists of str\n",
    "        List of references - and each reference is a list of tokens.\n",
    "        E.G. [[\"hola\", \"mundo\"], [\"alo\", \"tierra\"]]\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    int\n",
    "        The length of the hypothesis\n",
    "    int\n",
    "        The length of reference, which is closest to the hypothesis in length\n",
    "        \n",
    "    Note\n",
    "    ----\n",
    "    This just considers length in tokens - this doesn't consider N-grams.\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    # raise NotImplementedError()\n",
    "    hyplen = len(hypothesis)\n",
    "    \n",
    "    length_dic = {}\n",
    "    for ref in references:\n",
    "        if len(ref) in length_dic:\n",
    "            length_dic[len(ref)].append(ref)\n",
    "        else:\n",
    "            length_dic[len(ref)] = [ref]\n",
    "    \n",
    "    # find closet     \n",
    "    if hyplen in length_dic:\n",
    "        return hyplen, hyplen\n",
    "    else:\n",
    "        keys = list(length_dic.keys())\n",
    "        keys = np.abs(np.array(sorted(keys))- hyplen)\n",
    "        min_diff = min(keys)\n",
    "        \n",
    "        if hyplen - min_diff in length_dic:        \n",
    "            return hyplen, hyplen - min_diff\n",
    "        else:\n",
    "            return hyplen, hyplen + min_diff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "0d81ce1f3d284ab75009690e15caadf1",
     "grade": true,
     "grade_id": "cell-31315b4d20e99e6a",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "from nose.tools import assert_equal\n",
    "\n",
    "# Basic cases:\n",
    "assert_equal(brevity_penalty_values([\"a\",\"b\",\"c\"], [[\"e\",\"f\",\"g\"],[\"a\",\"b\"]]), (3,3))\n",
    "assert_equal(brevity_penalty_values([\"a\",\"b\",\"c\"], [[\"e\",\"f\",\"g\",\"h\"],[\"a\",\"b\"]]), (3,2))\n",
    "assert_equal(brevity_penalty_values([\"a\",\"b\",\"c\",\"d\"], [[\"e\",\"f\",\"g\",\"h\",\"i\",\"j\"],[\"a\",\"b\"]]), (4,2))\n",
    "assert_equal(brevity_penalty_values([\"a\",\"b\",\"c\",\"d\"], [[\"e\",\"f\",\"g\",\"h\"],[\"a\",\"b\"]]), (4,4))\n",
    "assert_equal(brevity_penalty_values([\"a\",\"b\"], [[\"e\",\"f\",\"g\",\"h\"],[\"a\",\"b\",\"c\",\"d\",\"e\"]]), (2,4))\n",
    "assert_equal(brevity_penalty_values([], [[\"a\",\"b\",\"c\",\"d\",\"e\"],[]]), (0,0))\n",
    "assert_equal(brevity_penalty_values([\"a\",\"b\",\"c\",\"d\",\"e\",\"f\",\"g\",\"h\",\"i\",\"j\",\"k\",\"l\",\"m\",\"n\",\"o\",\"p\",\"q\"],\n",
    "                                    [[\"r\",\"s\",\"t\",\"u\",\"v\",\"x\",\"y\"],\n",
    "                                     [\"a\",\"b\",\"c\",\"d\",\"e\",\"f\",\"g\",\"h\",\"i\",\"j\",\"k\",\"l\",\"m\",\"n\",\"o\",\"p\",\"q\",\"r\",\"s\",\"t\",\"u\",\"v\"]]), \n",
    "             (17, 22))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "488a7d02b8a0a849ffc466d4b603c59e",
     "grade": false,
     "grade_id": "cell-771b3a9511d24d16",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## 1.3 Put it together <a class=\"anchor\" id=\"subtask1_3\"></a>\n",
    "\n",
    "The key in BLEU is to aggregate the statistics from above over a large test corpus. BLEU computed on an individual sentence does not correlate well with human scores. Even if this sentence-wise BLEU is computed for all individual sentences in a test corpus and then averaged, the resulting metric is much less useful than when a single BLEU score is computed over the full corpus.\n",
    "\n",
    "The modified precision aggregation happens at each N-gram length separately. Typically, BLEU-4 is computed, meaning maximum N-gram length 4. In that case, 8 precision values are collected: total number of hits and total number of candidates at each each N.\n",
    "\n",
    "The length values just consider length in tokens, so only two values are needed: the total length of the hypotheses and the total length of the best matching reference lengths. The brevity penalty (BP) is then computed as follows:\n",
    "$$\n",
    "BP = 1,~c>r\n",
    "$$\n",
    "$$\n",
    "BP = exp(1-\\dfrac{r}{c}), c\\leq r\n",
    "$$\n",
    "with aggregated hypothesis length $c$ and aggregated closest reference length $r$.\n",
    "\n",
    "The metric is finally computed by:\n",
    "$$\n",
    "BLEU = BP\\times exp\\left( \\sum_n^N \\left[\\dfrac{1}{N}log(p_n)\\right]\\right)\n",
    "$$\n",
    "\n",
    "Here aggregated modified precision at N-gram length $n$ is simply computed as: $p_n = \\dfrac{\\text{Clipped Hits}}{\\text{Total Candidates}} $\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "9440b0b38a3363e2c0d8863bed6475b3",
     "grade": false,
     "grade_id": "cell-699f52568aa9243a",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Use these exp and log operations:\n",
    "from math import exp, log\n",
    "EPSILON = 1e-10  # This is used for smoothing zero counts (already implemented)\n",
    "\n",
    "def BLEU(hypothesis_set, references_set, max_n=4):\n",
    "    \"\"\"Computes BLEU over a corpus\n",
    "    \n",
    "    Use your two earlier functions.\n",
    "    \"\"\"\n",
    "\n",
    "    # Collect these statistics over the corpus ...\n",
    "    hits_totals = {N: 0 for N in range(1,max_n+1)} # Hits by N-gram N    \n",
    "    num_candidates_totals = {N: 0 for N in range(1,max_n+1)} # Num candidates by N\n",
    "    \n",
    "    hyplen_total = 0\n",
    "    closest_reflen_total = 0\n",
    "    # ... in the following loop:\n",
    "    for hypothesis, references in zip(hypothesis_set, references_set):\n",
    "        # YOUR CODE HERE\n",
    "        # raise NotImplementedError()\n",
    "        \n",
    "        hyplen, reflen = brevity_penalty_values(hypothesis, references)\n",
    "        hyplen_total += hyplen\n",
    "        closest_reflen_total += reflen\n",
    "        \n",
    "        for n in range(1,max_n+1):\n",
    "            clipped_hits, num_candidates = modified_precision(hypothesis, references, n)\n",
    "            # print(n, clipped_hits, num_candidates, hits_totals)\n",
    "            hits_totals[n] += clipped_hits\n",
    "            num_candidates_totals[n] += num_candidates\n",
    "        \n",
    "    # Additionally in BLEU, substituting a low value for zero counts is used\n",
    "    # to avoid log(0) problems.\n",
    "    # Smooth zero counts:\n",
    "    hits_totals = {N: hits if hits > 0 else EPSILON for N, hits in hits_totals.items()}\n",
    "    \n",
    "    # Now, compute brevity penalty based on \n",
    "    # hyplen_total and closest_reflen_totals\n",
    "    # Corner case: if hyplen_total == 0: brevity_penalty = 0.\n",
    "    # YOUR CODE HERE\n",
    "    # raise NotImplementedError()\n",
    "    p = {}\n",
    "    for n in range(1,max_n+1):\n",
    "        p[n] = hits_totals[n] / num_candidates_totals[n]\n",
    "        \n",
    "    # Finally compute the precisions, and weight them uniformly by 1 / max_n\n",
    "    # Then multiply by brevity penalty and return the final score.\n",
    "    # YOUR CODE HERE\n",
    "    # raise NotImplementedError()\n",
    "        \n",
    "    if hyplen_total == 0:\n",
    "        bp = 0\n",
    "    elif hyplen_total > closest_reflen_total: \n",
    "        bp = 1\n",
    "    else:\n",
    "        bp = exp(1 - closest_reflen_total / hyplen_total)\n",
    "        \n",
    "    bleu = bp * exp(1/max_n * sum([log(x) for x in p.values()]))\n",
    "    return bleu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "d1847bfb3a819737add2c376052f7f0c",
     "grade": true,
     "grade_id": "cell-3317cee95e9eda22",
     "locked": true,
     "points": 3,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "from numpy.testing import assert_almost_equal\n",
    "from nose.tools import assert_equal\n",
    "\n",
    "# Basic corpus input:\n",
    "hyps = [[\"a\", \"b\", \"c\",\"d\"],\n",
    "        [\"colourless\", \"green\", \"ideas\"],\n",
    "       [\"a\",\"c\",\"d\"]]        \n",
    "references = [[[\"a\",\"b\",\"c\"], [\"a\",\"b\",\"c\",\"d\",\"e\"]], \n",
    "              [[\"ideas\", \"green\", \"colourless\", \"sleep\"],[\"colourless\", \"green\"]],\n",
    "              [[\"a\",\"b\",\"c\"], [\"a\",\"b\",\"c\",\"d\",\"e\"]]]\n",
    "assert_almost_equal(BLEU(hyps, references), 0.5873949094699213, 2)\n",
    "# More corpus input:\n",
    "hyps = [[\"waters\", \"destroyed\", \"perfume\"],\n",
    "        [\"the\", \"the\", \"the\", \"the\", \"the\", \"the\", \"the\"],\n",
    "        [\"it\", \"is\", \"a\", \"widely\", \"accepted\", \"truth\", \n",
    "         \"that\", \"an\", \"unmarried\", \"wealthy\", \"man\", \n",
    "         \"necessarily\", \"needs\", \"a\", \"wife\", \"alongside\", \"him\"]]        \n",
    "references = [[[\"the\", \"city\", \"of\", \"cologne\", \"was\", \"destroyed\", \"by\", \"flood\", \"waters\"]], \n",
    "              [[\"the\", \"cat\", \"is\", \"on\", \"the\", \"mat\"], [\"there\", \"is\", \"a\", \"cat\", \"on\", \"the\", \"mat\"]],\n",
    "              [[\"it\",\"is\",\"a\",\"truth\",\"universally\",\"acknowledged\",\n",
    "                \"that\", \"a\", \"single\", \"man\", \"in\", \"posession\", \"of\",\"a\"\"good\",\"fortune\",\n",
    "               \"must\",\"be\",\"in\",\"want\",\"of\",\"a\",\"wife\"],\n",
    "               [\"everybody\", \"knows\", \"the\",\"rich\",\"man\",\"necessarily\", \"needs\", \"a\", \"wife\"]]]\n",
    "assert_almost_equal(BLEU(hyps, references), 0.1502348037292212, 2)\n",
    "# Exactly the same:\n",
    "assert_equal(BLEU([[1,2,3,4]], [[[1,2,3,4]]]), 1.0)\n",
    "# Slightly different:\n",
    "assert_almost_equal(BLEU([[5,1,2,3,4]], [[[1,2,3,4,5]]]), 0.7071067811865475, 2)\n",
    "# No hypothesis:\n",
    "assert_equal(BLEU([[]], [[[\"Silence\"]]]), 0.)\n",
    "# No hits at N=4:\n",
    "assert_almost_equal(BLEU([[1,2,3,4]], [[[1,2,3,5]]]), 0.0022360679774997894, 2)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "6616436f33e14adab81780a9023ae6c9",
     "grade": false,
     "grade_id": "cell-df2e90f797befcaf",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## 1.4 Reflect on BLEU <a class=\"anchor\" id=\"subtask1_4\"></a>\n",
    "Briefly answer the following questions:\n",
    "\n",
    "1. Will providing more references increase the BLEU score? Why?\n",
    "\n",
    "2. Why is the brevity penalty is used instead of the more traditional recall?\n",
    "\n",
    "3. Will a human translator always get a BLEU score of 1? Why?\n",
    "\n",
    "4. Autograding these written questions is impossible. Or is it? How could we use BLEU to autograde these answers? The upside would be to save a lot of teaching assistant time, but what would be some downsides?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "324538de940f16008e5f853338854de3",
     "grade": true,
     "grade_id": "cell-c038d749e95ea6d4",
     "locked": false,
     "points": 3,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "1 Yes. Because the more references there are, the more usages of different n-grams there are. This way, the system and evaluation method are more likely to identify similarities between the machine-translated sentence and the reference sentence, so as to achieve higher BLEU scores.   \n",
    "\n",
    "2 In general, a good translated sentence tends to be longer. And machine-translated sentences are often not as good as human-translated sentences. So, we can infer that machine translated sentences are usually shorter. And the recall metric tends to give inflated scores to shorter translations, which can lead to shorter machine-translated sentences. The brevity penalty can alleviate this problem by making machine-translated sentences longer.\n",
    "\n",
    "3 No. BLEU depends on how many n-gram both occur in both translated sentence and reference sentence. And even human-translated sentence is not guranteed to beable to use exactly same words/n-grams in reference sentences. \n",
    "\n",
    "4 BLEU can be used as a tool/additional assistant, but I do not think we can grade student's work alone. Because BLEU still have many limitations, like gramma, coherence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
